"""ReAct agent node implementation."""
from typing import List, Callable
import json
import re
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langchain_core.messages import ToolMessage, AIMessage, SystemMessage, HumanMessage
from .state import GraphState
from .common import log


# Legal document assistant system prompt
SYSTEM_PROMPT = """You are a legal document assistant specialized in retrieving and explaining Chinese law documents.

Core requirements:
1. Always ground every answer in retrieved documents. Do not rely on prior knowledge alone.
2. First call the collection_router tool to decide the collection, then use retrieve_hierarchical (and metadata/search tools if needed) before concluding.
3. Structure your final answer strictly into these four sections:
   - **å•é¡Œç­”æ¡ˆ**: Direct answer to the question, explaining the applicability and general context.
   - **å…·é«”æ¢æ–‡**: List specific articles with their content that support the answer. Use bullet points.
   - **çµè«–**: A concise summary of the findings.
   - **åƒè€ƒè³‡æ–™**: A list of all referenced documents and articles.
4. If no relevant documents are found, explicitly state that the knowledge base lacks information.

IMPORTANT INSTRUCTIONS:
- Do not output internal thought processes in <think> tags.
- After receiving the collection name from 'collection_router', you MUST immediately call 'retrieve_hierarchical' with the query and the collection name to get the document content.
- Do not stop until you have retrieved the actual text of the law.

### ONE-SHOT EXAMPLE (Follow this behavior):

User: "ã€Šé™¸æµ·ç©ºè»æ‡²ç½°æ³•ã€‹çš„é©ç”¨å°è±¡æ˜¯èª°ï¼Ÿ"
Assistant: (Calls tool 'collection_router' with query="é™¸æµ·ç©ºè»æ‡²ç½°æ³•")
Tool Output: "é™¸æµ·ç©ºè»æ‡²ç½°æ³•"
Assistant: (Calls tool 'retrieve_hierarchical' with query="é©ç”¨å°è±¡", collection="é™¸æµ·ç©ºè»æ‡²ç½°æ³•")
Tool Output: "...Article 2 content... Article 4 content..."
Assistant:

ã€Šé™¸æµ·ç©ºè»æ‡²ç½°æ³•ã€‹è¦å®šï¼Œè©²æ³•çš„é©ç”¨å°è±¡ç‚ºã€Œè»äººã€åŠå…¶åœ¨æœç¾å½¹æœŸé–“æ‰€çŠ¯ä¹‹é•ç´€è¡Œç‚ºã€‚

**å…·é«”æ¢æ–‡**
- **ç¬¬ 2 æ¢**
  1. ã€Œè»äººé•ç´€è¡Œç‚ºä¹‹æ‡²ç½°ï¼Œé™¤å…¶ä»–æ³•å¾‹å¦æœ‰è¦å®šå¤–ï¼Œä¾æœ¬æ³•è¡Œä¹‹ã€‚ã€
  2. ã€Œæœ¬æ³•è¦å®šï¼Œå°å–ªå¤±ç¾å½¹è»äººèº«åˆ†è€…æ–¼æœç¾å½¹æœŸé–“ä¹‹è¡Œç‚ºï¼Œäº¦é©ç”¨ä¹‹ã€‚ã€
  é€™æ®µæ–‡å­—æ˜ç¢ºæŒ‡å‡ºï¼Œä»»ä½•åœ¨æœç¾å½¹æœŸé–“æ‰€çŠ¯çš„é•ç´€è¡Œç‚ºï¼Œéƒ½å±¬æ–¼æœ¬æ³•çš„é©ç”¨ç¯„åœã€‚

- **ç¬¬ 4 æ¢**ï¼ˆç”¨è©å®šç¾©ï¼‰
  1. ã€Œé•ç´€è¡Œç‚ºï¼šæŒ‡è»äººæ–¼æœç¾å½¹æœŸé–“ï¼Œé•åå‹¤å‹™ä¸Šæˆ–å‹¤å‹™å¤–ç´€å¾‹è¦ç¯„ä¹‹è¡Œç‚ºã€‚ã€

**çµè«–**
ã€Šé™¸æµ·ç©ºè»æ‡²ç½°æ³•ã€‹ä¸»è¦é‡å°åœ¨æœç¾å½¹æœŸé–“çš„è»äººï¼ˆåŒ…æ‹¬å·²å–ªå¤±ç¾å½¹èº«åˆ†ä½†è¡Œç‚ºç™¼ç”Ÿæ–¼æœå½¹æœŸé–“è€…ï¼‰æ‰€çŠ¯çš„é•ç´€è¡Œç‚ºï¼Œä¸¦ä¾å…¶èº«åˆ†è¦å®šç›¸æ‡‰çš„æ‡²ç½°æªæ–½ã€‚

**åƒè€ƒè³‡æ–™**
- ä¾†æº: é™¸æµ·ç©ºè»æ‡²ç½°æ³•.md
  - æ¢æ–‡: ç¬¬ 2 æ¢
  - æ¢æ–‡: ç¬¬ 4 æ¢

Follow a ReAct style reasoning loop: think â†’ choose tool â†’ observe â†’ repeat â†’ final answer."""


def _clean_think_tags(text: str) -> str:
    """Remove <think>...</think> blocks from text."""
    if not text:
        return ""
    return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()


def _build_standard_format(tool_responses, ai_responses):
    """Build standard formatted output for tool responses."""
    answer_parts = ["# ğŸ¯ æŸ¥è©¢çµæœ\n"]
    answer_parts.append("æ ¹æ“šæ‚¨çš„æŸ¥è©¢,ä»¥ä¸‹æ˜¯å„å·¥å…·åŸ·è¡Œçµæœ:\n")
    
    for idx, tr in enumerate(tool_responses, 1):
        tool_name = tr['name']
        tool_content = tr['content']
        
        answer_parts.append(f"\n## {idx}. ã€{tool_name}ã€‘\n")
        
        try:
            data = json.loads(tool_content)
            if isinstance(data, dict):
                if 'error' in data:
                    answer_parts.append(f"âš ï¸ éŒ¯èª¤: {data['error']}\n")
                else:
                    for key, value in data.items():
                        if key.startswith('_'):
                            continue
                        if isinstance(value, dict):
                            answer_parts.append(f"\n**{key}**:\n")
                            for k, v in value.items():
                                answer_parts.append(f"  - {k}: {v}\n")
                        elif isinstance(value, list):
                            answer_parts.append(f"**{key}**: {value}\n")
                        else:
                            answer_parts.append(f"**{key}** = {value}\n")
            else:
                answer_parts.append(str(data))
        except json.JSONDecodeError:
            answer_parts.append(tool_content)
        
        answer_parts.append("\n---\n")
    
    if ai_responses:
        answer_parts.append("\n## ğŸ’¡ è£œå……èªªæ˜:\n")
        for ai_resp in ai_responses:
            answer_parts.append(ai_resp + "\n")
    
    answer_parts.append(f"\nâœ… å…±åŸ·è¡Œäº† {len(tool_responses)} å€‹å·¥å…·,å®ŒæˆæŸ¥è©¢ã€‚\n")
    
    return "".join(answer_parts)


def _extract_sources_from_text(text: str) -> List[str]:
    """Extract source entries from tool output text."""
    if not isinstance(text, str):
        return []

    sources: List[str] = []
    lines = text.splitlines()

    for idx, raw_line in enumerate(lines):
        line = raw_line.strip()
        if not line.startswith("ä¾†æº:"):
            continue

        entry = line.split("ä¾†æº:", 1)[1].strip()

        # Attach metadata information if present on the next line
        if idx + 1 < len(lines):
            next_line = lines[idx + 1].strip()
            if next_line.startswith("Metadata:"):
                metadata = next_line.split("Metadata:", 1)[1].strip()
                if metadata:
                    entry = f"{entry} ({metadata})"

        if entry and entry not in sources:
            sources.append(entry)

    return sources


def _collect_sources(tool_responses: List[dict]) -> List[str]:
    """Collect unique source entries from all tool responses."""
    collected: List[str] = []
    seen = set()

    for tr in tool_responses:
        entries = _extract_sources_from_text(tr.get('content', ""))
        for entry in entries:
            if entry not in seen:
                seen.add(entry)
                collected.append(entry)

    return collected


def _build_sources_section(source_entries: List[str]) -> str:
    """Build the citation section appended to final answers."""
    if not source_entries:
        return ""

    bullets = "\n".join(f"- ä¾†æº: {entry}" for entry in source_entries)
    return f"\n\nåƒè€ƒè³‡æ–™:\n{bullets}"


def create_agent_node(llm: ChatOpenAI, tools: List[Callable]) -> Callable:
    """Create a ReAct agent node for the workflow."""
    agent_executor = create_react_agent(
        llm,
        tools,
        prompt=SYSTEM_PROMPT
    )
    
    # Create a map for manual tool invocation
    tool_map = {t.name: t for t in tools}

    def agent_node(state: GraphState) -> dict:
        """ReAct agent node for general queries."""
        log("--- GENERAL AGENT NODE ---")

        question = state['question']
        messages_input = state['messages']

        if len(messages_input) > 4:
            messages_input = messages_input[-4:]

        try:
            result = agent_executor.invoke({
                "messages": messages_input
            })
            
            messages = result['messages']
            
            # --- FALLBACK LOGIC START ---
            # If the last action was collection_router, and no retrieval followed, force it.
            
            has_routed = False
            router_output = None
            has_retrieved = False
            
            for msg in messages:
                if isinstance(msg, ToolMessage):
                    if msg.name == 'collection_router':
                        has_routed = True
                        router_output = msg.content
                    elif msg.name in ('retrieve_hierarchical', 'retrieve_legal_documents'):
                        has_retrieved = True
            
            # If routed but not retrieved, manual intervention
            if has_routed and not has_retrieved and router_output and "éŒ¯èª¤" not in router_output:
                log("âš ï¸ Model stopped after routing. Forcing retrieval step manually.")
                
                # Determine which retrieval tool to use
                retrieval_tool_name = 'retrieve_hierarchical' if 'retrieve_hierarchical' in tool_map else 'retrieve_legal_documents'
                retrieval_tool = tool_map.get(retrieval_tool_name)
                
                if retrieval_tool:
                    log(f"Invoking {retrieval_tool_name} with collection='{router_output}'")
                    
                    # Clean the query by removing the collection name to improve retrieval precision
                    # e.g., "é™¸æµ·ç©ºè»æ‡²ç½°æ³•ç¬¬7æ¢" -> "ç¬¬7æ¢"
                    clean_query = question
                    try:
                        # Case-insensitive remove of collection name
                        clean_query = re.sub(re.escape(router_output), '', question, flags=re.IGNORECASE).strip()
                        if not clean_query: # If query becomes empty, revert to original
                            clean_query = question
                        elif len(clean_query) < 2 and len(question) > 5: # Too short, revert
                             clean_query = question
                    except Exception:
                        pass # Fallback to original question on error
                    
                    log(f"Refined manual query: '{question}' -> '{clean_query}'")

                    try:
                        # Prepare arguments based on tool signature
                        tool_args = {"query": clean_query}
                        if retrieval_tool_name == 'retrieve_hierarchical':
                            tool_args["collection"] = router_output
                        else:
                            tool_args["collection_name"] = router_output

                        # Invoke the tool manually
                        retrieval_result = retrieval_tool.invoke(tool_args)
                        
                        # Append the manual result to messages so the LLM can see it
                        manual_tool_msg = ToolMessage(
                            content=retrieval_result,
                            tool_call_id="manual_fallback_call",
                            name=retrieval_tool_name
                        )
                        messages.append(manual_tool_msg)
                        
                        # Re-invoke LLM to process the retrieved data
                        log("Re-invoking LLM to process manual retrieval results...")
                        # Prepend the system prompt so the LLM knows the format instructions
                        messages_with_prompt = [
                            SystemMessage(content=SYSTEM_PROMPT),
                            HumanMessage(content="è«‹æ ¹æ“šä¸Šè¿°å·¥å…·çš„åŸ·è¡Œçµæœï¼Œç¸½çµä¸¦å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼š\n\n**å•é¡Œç­”æ¡ˆ**\n[è©³ç´°å›ç­”]\n\n**å…·é«”æ¢æ–‡**\n[æ¢åˆ—å¼•ç”¨æ¢æ–‡å…§å®¹]\n\n**çµè«–**\n[ç°¡æ½”ç¸½çµ]\n\n**åƒè€ƒè³‡æ–™**\n- ä¾†æº: [æ–‡ä»¶åç¨±]\n  - æ¢æ–‡: [æ¢æ–‡è™Ÿç¢¼]\n")
                        ] + messages
                        fallback_response = llm.invoke(messages_with_prompt)
                        messages.append(fallback_response)
                        
                    except Exception as e:
                        log(f"Manual retrieval failed: {e}")

            # --- FALLBACK LOGIC END ---

            tool_responses = []
            ai_responses = []
            
            for i, msg in enumerate(messages):
                if isinstance(msg, ToolMessage):
                    tool_responses.append({
                        'name': getattr(msg, 'name', 'unknown_tool'),
                        'content': msg.content
                    })
                elif isinstance(msg, AIMessage) and msg.content.strip():
                    # Clean think tags from AI responses in history too if needed, 
                    # but mostly we care about the final answer.
                    content = _clean_think_tags(msg.content.strip())
                    if content:
                        ai_responses.append(content)

            if messages:
                last_msg = messages[-1]
                # log(f"DEBUG: Last message type: {type(last_msg)}")
                # log(f"DEBUG: Last message content (preview): {last_msg.content[:100]}")

            final_llm_answer = messages[-1].content if messages else ""
            final_llm_answer = _clean_think_tags(final_llm_answer)
            
            if not final_llm_answer.strip() or len(final_llm_answer.strip()) < 10:
                log("LLM final answer is empty or too short. Building answer from tool responses...")
                if tool_responses:
                    final_answer = _build_standard_format(tool_responses, ai_responses)
                else:
                    final_answer = "åŸ·è¡Œäº†æŸ¥è©¢,ä½†æ²’æœ‰ç²å¾—æœ‰æ•ˆçš„å·¥å…·å›æ‡‰çµæœã€‚"
            else:
                final_answer = final_llm_answer

            sources = _collect_sources(tool_responses)
            if sources and "åƒè€ƒè³‡æ–™" not in final_answer:
                final_answer = final_answer.rstrip() + _build_sources_section(sources)

            return {
                "generation": final_answer,
                "messages": messages
            }

        except Exception as e:
            error_msg = f"è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            log(f"ERROR in agent_node: {error_msg}")
            import traceback
            log(f"Traceback: {traceback.format_exc()}")
            return {"generation": f"æŠ±æ­‰ï¼Œ{error_msg}"}

    return agent_node
